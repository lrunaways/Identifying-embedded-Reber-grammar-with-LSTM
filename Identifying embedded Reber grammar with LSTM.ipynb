{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle, seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following string generator is known as a Embedded Reber Grammar:\n",
    "<img src=\"images/embreber.gif\" style=\"width:700px;height:500px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reber_string(random_seed = 0):\n",
    "    np.random.seed(random_seed)\n",
    "    edge_char = np.random.choice(['T', 'P'])\n",
    "    reber_string = 'B' + edge_char + 'B'\n",
    "    states_dict = {'0': ['1', '2'], '1': ['1', '3'], '2': ['2', '4'], '3': ['2', '5'], '4': ['3', '5'], '5': ['6']}\n",
    "    output_dict = {'0->1': 'T', '0->2': 'P', '1->1': 'S', '1->3': 'X',  '2->2': 'T', \n",
    "                   '2->4': 'V', '3->2': 'X', '3->5': 'S', '4->3': 'P', '4->5': 'V', '5->6': 'E'}\n",
    "    current_state = '0'\n",
    "    ch = ''\n",
    "    while(ch != 'E'):\n",
    "        previous_state = current_state\n",
    "        current_state = np.random.choice(states_dict[current_state])\n",
    "        transition = previous_state + \"->\" + current_state\n",
    "        ch = output_dict[transition]\n",
    "        reber_string = reber_string + ch\n",
    "    reber_string = reber_string + edge_char + 'E'\n",
    "    return reber_string\n",
    "\n",
    "def generate_incorrect_reber_string(random_seed = 0):\n",
    "    alphabet = set(['B', 'T', 'P', 'S', 'X', 'V', 'E'])\n",
    "    reber_string = generate_reber_string(random_seed)\n",
    "    make_error_flg = 1\n",
    "    string_len = len(reber_string)\n",
    "    correct_elements_indices = set(np.arange(string_len))\n",
    "    while(make_error_flg == 1 and len(correct_elements_indices) >= 1):\n",
    "        err_index = np.random.choice(list(correct_elements_indices))\n",
    "        correct_elements_indices = correct_elements_indices - set([err_index])\n",
    "        incorrect_letters = list(alphabet - set([reber_string[err_index]]))\n",
    "        reber_string = reber_string[:err_index] + np.random.choice(incorrect_letters) + reber_string[err_index+1:]\n",
    "        make_error_flg = np.random.choice([0, 1])\n",
    "    return reber_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct string:    BTBPVPSETE\n",
      "incorrect string:  BTBBVPBETE\n"
     ]
    }
   ],
   "source": [
    "print(\"correct string:   \", generate_reber_string())\n",
    "print(\"incorrect string: \", generate_incorrect_reber_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_n_strings(n, string_gen_func):\n",
    "    max_int = np.iinfo(np.int32).max\n",
    "    return [string_gen_func(np.random.randint(max_int)) for i in range(n)]\n",
    "\n",
    "\n",
    "def get_seq_lengths_from_X(X):\n",
    "    seq_lengths = [len(x) for x in X]\n",
    "    return seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train length: 30000 \n",
      "y_train shape: (30000,)\n",
      "X_test length: 30000 \n",
      "y_test shape: (30000,)\n"
     ]
    }
   ],
   "source": [
    "#set n correct & n incorrect strings\n",
    "n_correct_train = 15000\n",
    "n_incorrect_train = 15000\n",
    "n_correct_test = 15000\n",
    "n_incorrect_test = 15000\n",
    "n_correct = n_correct_train + n_correct_test\n",
    "n_incorrect = n_incorrect_train + n_incorrect_test\n",
    "n_train = n_correct_train + n_incorrect_train\n",
    "\n",
    "#generate correct & incorrect strings\n",
    "X_all = np.concatenate([gen_n_strings(n_correct, generate_reber_string),\n",
    "                        gen_n_strings(n_incorrect, generate_incorrect_reber_string)])\n",
    "y_all = np.hstack((np.ones(n_correct), np.zeros(n_incorrect))).astype(int)\n",
    "\n",
    "#shuffle data\n",
    "seed(0)            \n",
    "shuffle(X_all)\n",
    "seed(0)\n",
    "shuffle(y_all)\n",
    "X_train, y_train = X_all[:n_train], y_all[:n_train]\n",
    "X_test, y_test = X_all[n_train:], y_all[n_train:]\n",
    "print('\\nX_train length:', len(X_train), '\\ny_train shape:', y_train.shape)\n",
    "print('X_test length:', len(X_test), '\\ny_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTBTXSETE 1\n",
      "BBSPTVETE 0\n",
      "BTBTXXTTTVVETE 1\n",
      "BTBPTVPXVPXTVTSTE 0\n",
      "BTBTXSETE 1\n",
      "BPBBXPSEPE 0\n",
      "BTBPVVETE 1\n",
      "BTSPTSPPTVVSVE 0\n",
      "BPBPTVPXVVEPP 0\n",
      "BTBTSXSETE 1\n"
     ]
    }
   ],
   "source": [
    "#check generated data!\n",
    "for i in range(10):\n",
    "    print(X_all[i], y_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq_length_train = get_seq_lengths_from_X(X_train)\n",
    "seq_length_test = get_seq_lengths_from_X(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: 30000 X_test shape: 30000\n",
      "y_train shape: (30000, 1) y_test shape: (30000, 1)\n"
     ]
    }
   ],
   "source": [
    "#convert to one-hot encoding\n",
    "one_hot_dict = {'B': [1,0,0,0,0,0,0], 'T': [0,1,0,0,0,0,0], 'P': [0,0,1,0,0,0,0],\n",
    "                'S': [0,0,0,1,0,0,0], 'X': [0,0,0,0,1,0,0], 'V': [0,0,0,0,0,1,0], 'E': [0,0,0,0,0,0,1]}\n",
    "X_train = [[one_hot_dict[ch] for ch in x] for x in X_train]\n",
    "X_test = [[one_hot_dict[ch] for ch in x] for x in X_test]\n",
    "    \n",
    "y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
    "y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
    "print('X_train shape:', len(X_train), 'X_test shape:', len(X_test))\n",
    "print('y_train shape:', y_train.shape, 'y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 0.38439107\n",
      "Accuracy on test data: 0.8958333 Accuracy on validation data: 0.8355\n",
      "Epoch: 2 \tLoss: 0.36810723\n",
      "Accuracy on test data: 0.8958333 Accuracy on validation data: 0.87516665\n",
      "Epoch: 3 \tLoss: 0.2761153\n",
      "Accuracy on test data: 0.8958333 Accuracy on validation data: 0.906\n",
      "Epoch: 4 \tLoss: 0.26540756\n",
      "Accuracy on test data: 0.8958333 Accuracy on validation data: 0.91646665\n",
      "Epoch: 5 \tLoss: 0.18664515\n",
      "Accuracy on test data: 0.9375 Accuracy on validation data: 0.94776666\n",
      "Epoch: 6 \tLoss: 0.11891573\n",
      "Accuracy on test data: 0.9583333 Accuracy on validation data: 0.95023334\n",
      "Epoch: 7 \tLoss: 0.09218042\n",
      "Accuracy on test data: 0.9166667 Accuracy on validation data: 0.95523334\n",
      "Epoch: 8 \tLoss: 0.06481909\n",
      "Accuracy on test data: 0.9583333 Accuracy on validation data: 0.96213335\n",
      "Epoch: 9 \tLoss: 0.12437004\n",
      "Accuracy on test data: 0.9375 Accuracy on validation data: 0.9486667\n",
      "Epoch: 10 \tLoss: 0.0244302\n",
      "Accuracy on test data: 1.0 Accuracy on validation data: 0.9820667\n"
     ]
    }
   ],
   "source": [
    "n_inputs = len(\"BTPSXVE\")\n",
    "n_neurons = 64\n",
    "n_outputs = 1\n",
    "\n",
    "n_epochs = 10\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "n_batches = int(np.ceil(len(X_train) / batch_size))\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default(): \n",
    "    with tf.name_scope(\"LSTM\"):\n",
    "        X = tf.placeholder(tf.float32, [None, None, n_inputs])\n",
    "        y = tf.placeholder(tf.float32, [None, 1])\n",
    "        seq_length = tf.placeholder(tf.int32, [None])\n",
    "        learning_rate = tf.placeholder(tf.float32)\n",
    "        \n",
    "        lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=n_neurons)\n",
    "        outputs, states = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32, sequence_length= seq_length, swap_memory= True)\n",
    "        logits = tf.layers.dense(states[0], n_outputs)\n",
    "        \n",
    "        xentropy= tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "        \n",
    "        y_pred = tf.cast(tf.greater(logits, 0.), tf.float32, name=\"y_pred\")\n",
    "        y_proba = tf.nn.sigmoid(logits, name=\"y_proba\")\n",
    "        \n",
    "        equality = tf.equal(y_pred, y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "        \n",
    "        training_op = optimizer.minimize(loss)\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "with tf.Session(graph = g) as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            batch_start_idx = batch_index * batch_size\n",
    "            batch_end_idx = batch_start_idx + batch_size\n",
    "            X_batch, y_batch, seq_length_batch = X_train[batch_start_idx: batch_end_idx],\\\n",
    "                                                 y_train[batch_start_idx: batch_end_idx],\\\n",
    "                                                 seq_length_train[batch_start_idx: batch_end_idx]\n",
    "            X_batch = np.array(pad(X_batch))\n",
    "            loss_val, acc_train, _=  sess.run([loss, accuracy, training_op],\\\n",
    "                                        feed_dict={X: X_batch, y: y_batch, seq_length: seq_length_batch, learning_rate: lr})\n",
    "\n",
    "        print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss_val)\n",
    "        acc_val = sess.run(accuracy, feed_dict = {X: np.array(pad(X_test)), y: y_test, seq_length: seq_length_test} )\n",
    "        print(\"Accuracy on test data:\", acc_train, \"Accuracy on validation data:\", acc_val)\n",
    "    saver.save(sess, \"./my_reber_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_sample = [1]\n",
      "INFO:tensorflow:Restoring parameters from ./my_reber_classifier\n",
      "Estimated probability that BTBPTVPSETE is an Embedded Reber strings: 98.58%\n"
     ]
    }
   ],
   "source": [
    "#Test random string\n",
    "random_number = np.random.randint(low=999999, high=np.iinfo(np.int32).max)\n",
    "y_sample = [np.random.randint(2)]\n",
    "print('y_sample =', y_sample)\n",
    "if y_sample == [1]:\n",
    "    init_string_sample = generate_reber_string(random_number)\n",
    "else: \n",
    "    init_string_sample = generate_incorrect_reber_string(random_number)\n",
    "X_sample = np.array([one_hot_dict[ch] for ch in list(init_string_sample)])\n",
    "X_sample = np.reshape(X_sample, (1, -1, n_inputs))\n",
    "seq_length_sample = seq_lengths_from_X(X_sample)\n",
    "with tf.Session(graph = g) as sess:\n",
    "    saver.restore(sess, \"./my_reber_classifier\");\n",
    "    y_proba_sample = y_proba.eval(feed_dict={X: X_sample, seq_length: np.array(seq_length_sample)})\n",
    "print(\"Estimated probability that\", init_string_sample, \"is an Embedded Reber strings: {:.2f}%\".format(y_proba_sample[0][0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
