{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle, seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following string generator is known as a Embedded Reber Grammar:\n",
    "<img src=\"images/embreber.gif\" style=\"width:700px;height:500px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedded_reber_string(random_seed = 0):\n",
    "    np.random.seed(random_seed)\n",
    "    edge_char = np.random.choice(['T', 'P'])\n",
    "    reber_string = 'B' + edge_char + 'B'\n",
    "    states_dict = {'0': ['1', '2'], '1': ['1', '3'], '2': ['2', '4'], '3': ['2', '5'], '4': ['3', '5'], '5': ['6']}\n",
    "    output_dict = {'0->1': 'T', '0->2': 'P', '1->1': 'S', '1->3': 'X',  '2->2': 'T', \n",
    "                   '2->4': 'V', '3->2': 'X', '3->5': 'S', '4->3': 'P', '4->5': 'V', '5->6': 'E'}\n",
    "    current_state = '0'\n",
    "    ch = ''\n",
    "    while(ch != 'E'):\n",
    "        previous_state = current_state\n",
    "        current_state = np.random.choice(states_dict[current_state])\n",
    "        transition = previous_state + \"->\" + current_state\n",
    "        ch = output_dict[transition]\n",
    "        reber_string = reber_string + ch\n",
    "    reber_string = reber_string + edge_char + 'E'\n",
    "    return reber_string\n",
    "\n",
    "def generate_incorrect_reber_string(random_seed = 0):\n",
    "    alphabet = set(['B', 'T', 'P', 'S', 'X', 'V', 'E'])\n",
    "    reber_string = generate_embedded_reber_string(random_seed)\n",
    "    make_error_flg = 1\n",
    "    string_len = len(reber_string)\n",
    "    correct_elements_indices = set(np.arange(string_len))\n",
    "    while(make_error_flg == 1 and len(correct_elements_indices) >= 1):\n",
    "        err_index = np.random.choice(list(correct_elements_indices))\n",
    "        correct_elements_indices = correct_elements_indices - set([err_index])\n",
    "        incorrect_letters = list(alphabet - set([reber_string[err_index]]))\n",
    "        reber_string = reber_string[:err_index] + np.random.choice(incorrect_letters) + reber_string[err_index+1:]\n",
    "        make_error_flg = np.random.choice([0, 1])\n",
    "    return reber_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct string:    BTBPVPSETE\n",
      "incorrect string:  BTBTVPPETE\n"
     ]
    }
   ],
   "source": [
    "print(\"correct string:   \", generate_embedded_reber_string())\n",
    "print(\"incorrect string: \", generate_incorrect_reber_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_n_strings(n, start_seed, func):\n",
    "    X = []\n",
    "    for i in np.arange(n):\n",
    "        X.append(func(i+start_seed))\n",
    "        if(i % 25000 == 0):\n",
    "            print(i,'/', n)\n",
    "    print(n, '/', n)\n",
    "    return X\n",
    "\n",
    "def seq_lengths_from_X(X):\n",
    "    seq_lengths = [len(x) for x in X]\n",
    "    return seq_lengths\n",
    "\n",
    "def pad(X):\n",
    "    zeros_seq = [[0,0,0,0,0,0,0]]\n",
    "    max_len = max([len(x) for x in X])\n",
    "    for i in range(len(X)):\n",
    "        n_pads =  max_len - len(X[i]) \n",
    "        X[i].extend(zeros_seq*n_pads)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating 15000 correct strings...\n",
      "0 / 30000\n",
      "25000 / 30000\n",
      "30000 / 30000\n",
      "generating 15000 incorrect strings...\n",
      "0 / 30000\n",
      "25000 / 30000\n",
      "30000 / 30000\n",
      "\n",
      "X_train length: 30000 \n",
      "y_train shape: (30000,)\n",
      "X_test length: 30000 \n",
      "y_test shape: (30000,)\n"
     ]
    }
   ],
   "source": [
    "n_correct_train = 15000\n",
    "n_incorrect_train = 15000\n",
    "n_correct_test = 15000\n",
    "n_incorrect_test = 15000\n",
    "n_correct = n_correct_train + n_correct_test\n",
    "n_incorrect = n_incorrect_train + n_incorrect_test\n",
    "n_train = n_correct_train + n_incorrect_train\n",
    "y_all = np.hstack((np.ones(n_correct), np.zeros(n_incorrect)))\n",
    "X_all = []\n",
    "\n",
    "print('generating', n_correct_train, 'correct strings...')\n",
    "seed_idx = 0\n",
    "X_all.extend(gen_n_strings(n_correct, seed_idx, generate_embedded_reber_string))\n",
    "print('generating', n_incorrect_train, 'incorrect strings...')\n",
    "seed_idx = n_correct\n",
    "X_all.extend(gen_n_strings(n_incorrect, seed_idx, generate_incorrect_reber_string))\n",
    "seed(0)            #shuffle data\n",
    "shuffle(X_all)\n",
    "seed(0)\n",
    "shuffle(y_all)\n",
    "X_train, y_train = X_all[:n_train], y_all[:n_train]\n",
    "X_test, y_test = X_all[n_train:], y_all[n_train:]\n",
    "print('\\nX_train length:', len(X_train), '\\ny_train shape:', y_train.shape)\n",
    "print('X_test length:', len(X_test), '\\ny_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTBTSXSETE 1.0\n",
      "BPXPTVVEPE 0.0\n",
      "BPBTSSXSEPE 1.0\n",
      "BTBTSSSPXSETE 0.0\n",
      "BPBTXSEPE 1.0\n",
      "PTBPTTVVETE 0.0\n",
      "BPBPVPXVVEPE 1.0\n",
      "BTBPTVPETE 0.0\n",
      "BPEPVPXTVXXXE 0.0\n",
      "BPBTSXXTTTVPXVVEPE 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(X_all[i], y_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq_length_train = seq_lengths_from_X(X_train)\n",
    "seq_length_test = seq_lengths_from_X(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (30000, 1) y_test shape: (30000, 1)\n"
     ]
    }
   ],
   "source": [
    "one_hot_dict = {'B': [1,0,0,0,0,0,0], 'T': [0,1,0,0,0,0,0], 'P': [0,0,1,0,0,0,0],\n",
    "                'S': [0,0,0,1,0,0,0], 'X': [0,0,0,0,1,0,0], 'V': [0,0,0,0,0,1,0],\n",
    "                'E': [0,0,0,0,0,0,1]}\n",
    "#X_one_hot_all = np.zeros((len(X_all), max_len, len(one_hot_dict))) \n",
    "for i in range(len(X_train)):\n",
    "    X_train[i] = [one_hot_dict[ch] for ch in list(X_train[i])]\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i] = [one_hot_dict[ch] for ch in list(X_test[i])]\n",
    "    \n",
    "#X_train = np.array(pad(X_train))\n",
    "#X_test = np.array(pad(X_test))\n",
    "y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
    "y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
    "#print('X_train shape:', X_train.shape, 'X_test shape:', X_test.shape)\n",
    "print('y_train shape:', y_train.shape, 'y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 0.560031\n",
      "Accuracy on test data: 0.770833 Accuracy on validation data: 0.795933\n",
      "Epoch: 2 \tLoss: 0.430693\n",
      "Accuracy on test data: 0.833333 Accuracy on validation data: 0.8529\n",
      "Epoch: 3 \tLoss: 0.347088\n",
      "Accuracy on test data: 0.875 Accuracy on validation data: 0.8983\n",
      "Epoch: 4 \tLoss: 0.305636\n",
      "Accuracy on test data: 0.895833 Accuracy on validation data: 0.918833\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9d8619db9651>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mX_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             loss_val, acc_train, _=  sess.run([loss, accuracy, training_op],\\\n\u001b[1;32m---> 42\u001b[1;33m                                         feed_dict={X: X_batch, y: y_batch, seq_length: seq_length_batch, learning_rate: lr})\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\tLoss:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\anaconda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\anaconda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\anaconda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\anaconda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programs\\anaconda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_inputs = len(\"BTPSXVE\")\n",
    "n_neurons = 64\n",
    "n_outputs = 1\n",
    "\n",
    "n_epochs = 35\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "n_batches = int(np.ceil(len(X_train) / batch_size))\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default(): \n",
    "    with tf.name_scope(\"LSTM\"):\n",
    "        X = tf.placeholder(tf.float32, [None, None, n_inputs])\n",
    "        y = tf.placeholder(tf.float32, [None, 1])\n",
    "        seq_length = tf.placeholder(tf.int32, [None])\n",
    "        learning_rate = tf.placeholder(tf.float32)\n",
    "        lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=n_neurons)\n",
    "        outputs, states = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32, sequence_length= seq_length, swap_memory= True)\n",
    "        logits = tf.layers.dense(states[0], n_outputs)\n",
    "        xentropy= tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "        y_pred = tf.cast(tf.greater(logits, 0.), tf.float32, name=\"y_pred\")\n",
    "        y_proba = tf.nn.sigmoid(logits, name=\"y_proba\")\n",
    "        equality = tf.equal(y_pred, y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "        training_op = optimizer.minimize(loss)\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "with tf.Session(graph = g) as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            batch_start_idx = batch_index * batch_size\n",
    "            batch_end_idx = batch_start_idx + batch_size\n",
    "            X_batch, y_batch, seq_length_batch = X_train[batch_start_idx: batch_end_idx],\\\n",
    "                                                 y_train[batch_start_idx: batch_end_idx],\\\n",
    "                                                 seq_length_train[batch_start_idx: batch_end_idx]\n",
    "            X_batch = np.array(pad(X_batch))\n",
    "            loss_val, acc_train, _=  sess.run([loss, accuracy, training_op],\\\n",
    "                                        feed_dict={X: X_batch, y: y_batch, seq_length: seq_length_batch, learning_rate: lr})\n",
    "\n",
    "        print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss_val)\n",
    "        acc_val = sess.run(accuracy, feed_dict = {X: np.array(pad(X_test)), y: y_test, seq_length: seq_length_test} )\n",
    "        print(\"Accuracy on test data:\", acc_train, \"Accuracy on validation data:\", acc_val)\n",
    "    saver.save(sess, \"./my_reber_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_sample = [0]\n",
      "INFO:tensorflow:Restoring parameters from ./my_reber_classifier\n",
      "Estimated probability that BTBPTTVVEPE is an Embedded Reber strings: 0.56%\n"
     ]
    }
   ],
   "source": [
    "random_number = np.random.randint(100000, high = 99999999)\n",
    "y_sample = [np.random.randint(2)]\n",
    "print('y_sample =', y_sample)\n",
    "if y_sample == [1]:\n",
    "    init_string_sample = generate_embedded_reber_string(random_number)\n",
    "else: \n",
    "    init_string_sample = generate_incorrect_reber_string(random_number)\n",
    "X_sample = np.array([one_hot_dict[ch] for ch in list(init_string_sample)])\n",
    "X_sample = np.reshape(X_sample, (1, -1, n_inputs))\n",
    "seq_length_sample = seq_lengths_from_X(X_sample)\n",
    "with tf.Session(graph = g) as sess:\n",
    "    saver.restore(sess, \"./my_reber_classifier\");\n",
    "    y_proba_sample = y_proba.eval(feed_dict={X: X_sample, seq_length: np.array(seq_length_sample)})\n",
    "print(\"Estimated probability that\", init_string_sample, \"is an Embedded Reber strings: {:.2f}%\".format(y_proba_sample[0][0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
